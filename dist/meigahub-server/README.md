# ‚ö° MeigaHub-server ‚Äî Texto + Audio con gesti√≥n de VRAM

> Servidor proxy FastAPI compatible con la **API de OpenAI** que alterna autom√°ticamente entre un backend de **texto (llama.cpp)** y uno de **audio (whisper.cpp)**, liberando VRAM de GPU cada vez que cambia de modo.

---

## üìñ ¬øQu√© hace esta aplicaci√≥n?

Este servidor act√∫a como **proxy inteligente** entre tus aplicaciones y dos backends locales de IA:

| Funci√≥n | Backend | Tecnolog√≠a |
|---|---|---|
| Chat, completions, embeddings, responses | **LLM** | [llama.cpp](https://github.com/ggerganov/llama.cpp) |
| Transcripci√≥n y traducci√≥n de audio | **Whisper** | [whisper.cpp](https://github.com/ggerganov/whisper.cpp) |

**Caracter√≠sticas principales:**

- üîÑ **Cambio autom√°tico de backend** ‚Äî cuando llega una petici√≥n de texto arranca LLM; cuando llega una de audio arranca Whisper y apaga LLM (y viceversa), liberando VRAM.
- üß† **Cambio din√°mico de modelo LLM** ‚Äî puedes enviar el campo `"model"` en la petici√≥n y el servidor recargar√° llama-server con ese modelo GGUF autom√°ticamente.
- üñ•Ô∏è **Interfaz web de gesti√≥n de modelos** ‚Äî busca, descarga y elimina modelos GGUF desde Hugging Face directamente desde el navegador.
- üéÆ **Indicador de VRAM en tiempo real** ‚Äî la UI muestra la GPU detectada, VRAM disponible y si cada modelo cabe en tu tarjeta.
- üì° **Compatible con la API de OpenAI** ‚Äî usa los mismos endpoints `/v1/*` que cualquier cliente OpenAI-compatible.

---

## üìÅ Estructura de carpetas del sistema completo

```
C:\
‚îú‚îÄ‚îÄ apps\                              ‚Üê Ejecutables de los backends
‚îÇ   ‚îú‚îÄ‚îÄ llama.cpp\                     ‚Üê llama.cpp (CPU)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llama-server.exe
‚îÇ   ‚îú‚îÄ‚îÄ llama.cpp-cuda\                ‚Üê llama.cpp con aceleraci√≥n CUDA (GPU NVIDIA)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llama-server.exe
‚îÇ   ‚îî‚îÄ‚îÄ whisper.cpp\
‚îÇ       ‚îî‚îÄ‚îÄ Release\
‚îÇ           ‚îî‚îÄ‚îÄ whisper-server.exe     ‚Üê whisper.cpp server
‚îÇ
‚îú‚îÄ‚îÄ models\                            ‚Üê Modelos GGUF (LLM y Whisper)
‚îÇ   ‚îú‚îÄ‚îÄ mistral-7b-instruct-v0.2.Q4_0.gguf
‚îÇ   ‚îú‚îÄ‚îÄ qwen2.5-7b-instruct-q4_k_m.gguf
‚îÇ   ‚îú‚îÄ‚îÄ Qwen3-14B.Q6_K.gguf
‚îÇ   ‚îî‚îÄ‚îÄ ggml-medium.bin                ‚Üê modelo Whisper
‚îÇ
‚îî‚îÄ‚îÄ Users\<tu-usuario>\Desktop\
    ‚îî‚îÄ‚îÄ servidordellm\                 ‚Üê ESTE PROYECTO (MeigaHub-server)
        ‚îú‚îÄ‚îÄ .env                       ‚Üê Configuraci√≥n local (no se sube a git)
        ‚îú‚îÄ‚îÄ .env.example               ‚Üê Plantilla de configuraci√≥n
        ‚îú‚îÄ‚îÄ requirements.txt           ‚Üê Dependencias Python
        ‚îú‚îÄ‚îÄ README.md
        ‚îú‚îÄ‚îÄ app\                       ‚Üê C√≥digo fuente del servidor
        ‚îÇ   ‚îú‚îÄ‚îÄ main.py                ‚Üê Endpoints FastAPI + proxy + UI
        ‚îÇ   ‚îú‚îÄ‚îÄ config.py              ‚Üê Configuraci√≥n desde .env (Pydantic)
        ‚îÇ   ‚îú‚îÄ‚îÄ backend_manager.py     ‚Üê Arranque/parada/cambio de backends
        ‚îÇ   ‚îî‚îÄ‚îÄ model_manager.py       ‚Üê Gesti√≥n de modelos + API de Hugging Face
        ‚îî‚îÄ‚îÄ scripts\                   ‚Üê Utilidades de inicio/parada/test
            ‚îú‚îÄ‚îÄ start_server.bat       ‚Üê Doble clic para iniciar
            ‚îú‚îÄ‚îÄ stop_server.bat        ‚Üê Doble clic para detener
            ‚îú‚îÄ‚îÄ start_server.ps1       ‚Üê PowerShell start/stop
            ‚îú‚îÄ‚îÄ stop_server.ps1        ‚Üê Alias para stop
            ‚îú‚îÄ‚îÄ setup_check.ps1        ‚Üê Verifica que todo est√© en su sitio
            ‚îî‚îÄ‚îÄ test_llm.py            ‚Üê Smoke test de endpoints LLM
```

---

## üîß Requisitos previos

| Requisito | Detalle |
|---|---|
| **Sistema operativo** | Windows 10/11 (64-bit) |
| **Python** | 3.10 o superior ([python.org](https://www.python.org/downloads/)) |
| **GPU (recomendado)** | NVIDIA con drivers actualizados y CUDA (para versi√≥n CUDA de llama.cpp) |
| **RAM** | M√≠nimo 8 GB (16 GB recomendado para modelos 7B) |
| **Espacio en disco** | ~50 MB para el servidor + espacio para modelos GGUF (2‚Äì10 GB cada uno) |

---

## üöÄ Instalaci√≥n paso a paso

### 1. Instalar llama.cpp (backend LLM)

llama.cpp es el motor que ejecuta los modelos de lenguaje GGUF.

**Opci√≥n A ‚Äî Descargar binarios precompilados (recomendado):**

1. Ve a [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases).
2. Descarga la versi√≥n adecuada:
   - **Con GPU NVIDIA (CUDA):** `llama-<version>-bin-win-cuda-cu12.2.0-x64.zip` (o la versi√≥n CUDA de tu driver).
   - **Solo CPU:** `llama-<version>-bin-win-x64.zip`
3. Extrae el contenido en una carpeta, por ejemplo:
   - `C:\apps\llama.cpp-cuda\` (versi√≥n CUDA)
   - `C:\apps\llama.cpp\` (versi√≥n CPU)
4. Verifica que existe `llama-server.exe` dentro de la carpeta.

**Opci√≥n B ‚Äî Compilar desde fuente:**

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build -DGGML_CUDA=ON    # usar -DGGML_CUDA=OFF para solo CPU
cmake --build build --config Release
```

### 2. Instalar whisper.cpp (backend Audio)

whisper.cpp es el motor que ejecuta los modelos de transcripci√≥n de audio.

**Opci√≥n A ‚Äî Descargar binarios precompilados:**

1. Ve a [whisper.cpp releases](https://github.com/ggerganov/whisper.cpp/releases).
2. Descarga la versi√≥n para Windows (ej: `whisper-<version>-bin-x64.zip`).
3. Extrae en `C:\apps\whisper.cpp\Release\`.
4. Verifica que existe `whisper-server.exe`.

**Opci√≥n B ‚Äî Compilar desde fuente:**

```bash
git clone https://github.com/ggerganov/whisper.cpp
cd whisper.cpp
cmake -B build
cmake --build build --config Release
```

### 3. Descargar modelos GGUF

Los modelos son archivos pesados que **no se incluyen en el instalador**. Col√≥calos en `C:\models\` (o la ruta que configures).

**Modelos LLM (texto):**
- Busca modelos GGUF en [Hugging Face](https://huggingface.co/models?search=gguf).
- Recomendaciones para empezar:
  - `mistral-7b-instruct-v0.2.Q4_0.gguf` (~4 GB, bueno para 8 GB VRAM)
  - `qwen2.5-7b-instruct-q4_k_m.gguf` (~4.5 GB)
- Tambi√©n puedes usar la **interfaz web del servidor** en `/ui/models` para buscar y descargar modelos directamente.

**Modelos Whisper (audio):**
- Descarga desde [Hugging Face - ggerganov/whisper.cpp](https://huggingface.co/ggerganov/whisper.cpp/tree/main).
- Recomendaciones:
  - `ggml-medium.bin` (~1.5 GB, buen balance velocidad/calidad)
  - `ggml-small.bin` (~466 MB, m√°s r√°pido)
  - `ggml-large-v3.bin` (~3 GB, mejor calidad)

### 4. Instalar MeigaHub-server (este proyecto)

```powershell
# Descargar desde GitHub
cd C:\Users\<tu-usuario>\Desktop
git clone https://github.com/xenkxp/MeigaHub-server.git servidordellm
cd servidordellm

# Instalar dependencias Python
py -3.12 -m pip install -r requirements.txt

# Copiar y editar la configuraci√≥n
copy .env.example .env
notepad .env
```

> **Sin git?** Ve a https://github.com/xenkxp/MeigaHub-server ‚Üí bot√≥n **Code** ‚Üí **Download ZIP**, extrae y contin√∫a desde `cd servidordellm`.

### 5. Configurar `.env`

Edita el archivo `.env` con las rutas reales de tu sistema:

```ini
# === SERVIDOR ===
SERVER_HOST=0.0.0.0
SERVER_PORT=3112

# === LLM (llama.cpp) ===
LLM_BACKEND_URL=http://127.0.0.1:8082
LLM_MODEL_NAME=mistral-7b-instruct-v0.2.Q4_0.gguf
LLM_START_COMMAND=C:\apps\llama.cpp-cuda\llama-server.exe --port 8082 --model C:\models\mistral-7b-instruct-v0.2.Q4_0.gguf --embeddings --pooling mean
LLM_STOP_COMMAND=taskkill /IM llama-server.exe /F

# === WHISPER (whisper.cpp) ===
WHISPER_BACKEND_URL=http://127.0.0.1:8081
WHISPER_MODEL_NAME=ggml-medium.bin
WHISPER_START_COMMAND=C:\apps\whisper.cpp\Release\whisper-server.exe --port 8081 --model C:\models\ggml-medium.bin
WHISPER_STOP_COMMAND=taskkill /IM whisper-server.exe /F

# === OPCIONES ===
AUTO_SWITCH_BACKEND=true          # Cambiar backend autom√°ticamente seg√∫n endpoint
RESPONSES_MODE=map                # "map" redirige /v1/responses ‚Üí /v1/chat/completions
                                  # "proxy" reenv√≠a directamente a /v1/responses del backend

# === MODELOS ===
MODELS_DIR=C:\models              # Carpeta donde se almacenan los .gguf
HF_TOKEN=                         # Token de Hugging Face (opcional, para repos privados)
MODELS_LIST_MODE=both             # "active" | "local" | "both"

# === HEALTH CHECKS ===
LLM_HEALTH_PATH=/v1/models
WHISPER_HEALTH_PATH=/v1/models
SWITCH_TIMEOUT_SECONDS=30
```

### 6. Verificar la instalaci√≥n

```powershell
powershell -ExecutionPolicy Bypass -File scripts\setup_check.ps1
```

Esto comprobar√° que existen todos los ejecutables, modelos y carpetas necesarias.

---

## ‚ñ∂Ô∏è Iniciar y detener el servidor

**Con doble clic (m√°s f√°cil):**
- Iniciar: doble clic en `scripts\start_server.bat`
- Detener: doble clic en `scripts\stop_server.bat`

**Desde PowerShell:**
```powershell
# Iniciar
powershell -ExecutionPolicy Bypass -File scripts\start_server.ps1 -Action start

# Detener
powershell -ExecutionPolicy Bypass -File scripts\start_server.ps1 -Action stop
```

**Desde terminal directamente:**
```powershell
py -3.12 -m uvicorn app.main:app --host 0.0.0.0 --port 3112
```

Una vez iniciado, el servidor estar√° disponible en: `http://127.0.0.1:3112`

---

## üì° Endpoints ‚Äî Referencia completa

### Endpoints compatibles con OpenAI (requieren backend LLM)

| M√©todo | Ruta | Descripci√≥n |
|---|---|---|
| `GET` | `/v1/models` | Lista modelos disponibles (compatible OpenAI). Comportamiento seg√∫n `MODELS_LIST_MODE`. |
| `POST` | `/v1/chat/completions` | Chat completions (formato OpenAI). Soporta campo `"model"` para cambio din√°mico. |
| `POST` | `/v1/completions` | Text completions cl√°sico. Soporta campo `"model"`. |
| `POST` | `/v1/embeddings` | Genera embeddings del texto (requiere `--embeddings` en llama-server). |
| `POST` | `/v1/responses` | Responses API. Se redirige a chat/completions (`RESPONSES_MODE=map`) o al backend directo (`proxy`). |

### Endpoints de audio (requieren backend Whisper)

| M√©todo | Ruta | Descripci√≥n |
|---|---|---|
| `POST` | `/v1/audio/transcriptions` | Transcribe audio a texto (formato OpenAI). Par√°metros: `file`, `model`, `language`, `prompt`, `response_format`, `temperature`. |
| `POST` | `/v1/audio/translations` | Traduce audio a texto en ingl√©s. Mismos par√°metros que transcriptions (sin `language`). |

### Endpoints del servidor (siempre disponibles)

| M√©todo | Ruta | Descripci√≥n |
|---|---|---|
| `GET` | `/status` | Estado del servidor: backend activo, modelo cargado, VRAM, si est√° ocupado cambiando. |
| `GET` | `/ui/gpu` | Info de GPU NVIDIA: nombre, VRAM total/libre/usada (JSON). |
| `GET` | `/debug/routes` | Lista todas las rutas registradas en el servidor. |
| `GET/POST/PUT/PATCH/DELETE` | `/debug/echo` | Devuelve info de la petici√≥n recibida (para debug). |

### Endpoints de la interfaz de gesti√≥n de modelos

| M√©todo | Ruta | Descripci√≥n |
|---|---|---|
| `GET` | `/ui/models` | **P√°gina web** del gestor de modelos GGUF (abrir en navegador). |
| `GET` | `/ui/models/search?q=mistral&only_gguf=1&limit=12` | Busca repos en Hugging Face. |
| `GET` | `/ui/models/files?repo=TheBloke/Mistral-7B-Instruct-v0.2-GGUF` | Lista archivos GGUF de un repo con tama√±os. |
| `GET` | `/ui/models/local` | Lista modelos GGUF locales en `MODELS_DIR` con tama√±os. |
| `DELETE` | `/ui/models/local` | Borra un modelo local. Body: `{"name": "archivo.gguf"}`. |
| `POST` | `/ui/models/download` | Inicia descarga de un GGUF. Body: `{"repo": "...", "file": "..."}`. Devuelve `{"id": "job-uuid"}`. |
| `GET` | `/ui/models/download/{job_id}` | Consulta progreso de descarga: `status`, `downloaded_bytes`, `total_bytes`. |

---

## üñ•Ô∏è Interfaz web ‚Äî Gestor de modelos

Abre en tu navegador: **http://127.0.0.1:3112/ui/models**

La interfaz tiene tres pesta√±as:

| Pesta√±a | Funci√≥n |
|---|---|
| üîé **Buscar** | Busca modelos en Hugging Face. Filtra por repos con archivos GGUF. Clic en un resultado para ir a descargar. |
| üì¶ **Descargar** | Introduce un repo de HF, lista sus archivos GGUF, muestra tama√±os y estimaci√≥n de VRAM. El archivo recomendado (‚≠ê) es el m√°s grande que cabe en tu GPU. Barra de progreso en tiempo real. |
| üíæ **Locales** | Lista todos los modelos `.gguf` en tu carpeta `MODELS_DIR`. Muestra tama√±o, VRAM estimada y permite borrar. |

La barra superior muestra:
- **Estado del backend** ‚Äî cu√°l est√° activo (LLM/Whisper/ninguno) y qu√© modelo tiene cargado.
- **Info de GPU** ‚Äî nombre de la tarjeta, VRAM total y libre.

---

## üîÑ Comportamiento del cambio de backend

```
Petici√≥n POST /v1/chat/completions
        ‚îÇ
        ‚ñº
   ¬øBackend LLM activo?
        ‚îÇ
   S√≠ ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚îÄ‚îÄ No
   ‚îÇ         ‚îÇ
   ‚îÇ    ¬øAUTO_SWITCH_BACKEND=true?
   ‚îÇ         ‚îÇ
   ‚îÇ    S√≠ ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚îÄ‚îÄ No ‚Üí Error 409
   ‚îÇ         ‚îÇ
   ‚îÇ    1. Detiene Whisper (libera VRAM)
   ‚îÇ    2. Inicia llama-server
   ‚îÇ    3. Espera health check OK
   ‚îÇ         ‚îÇ
   ‚ñº         ‚ñº
   Proxy ‚Üí llama-server ‚Üí Respuesta
```

- **Solo un backend activo a la vez** para maximizar VRAM disponible.
- El campo `"model"` en la petici√≥n permite cambiar de modelo LLM sin reiniciar el servidor.
- Si el backend ya est√° activo con el modelo correcto, la petici√≥n se reenv√≠a inmediatamente.
- Timeout configurable con `SWITCH_TIMEOUT_SECONDS` (default: 30s).

---

## üìã Ejemplos de uso

### Chat completions
```bash
curl http://127.0.0.1:3112/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"mistral-7b-instruct-v0.2.Q4_0.gguf","messages":[{"role":"user","content":"Hola"}],"max_tokens":100}'
```

### Transcripci√≥n de audio
```bash
curl http://127.0.0.1:3112/v1/audio/transcriptions \
  -F file=@audio.wav \
  -F language=es
```

### Estado del servidor
```bash
curl http://127.0.0.1:3112/status
# {"backend":"llm","model":"mistral-7b-instruct-v0.2.Q4_0.gguf","vram":"","busy":false}
```

### Con Python (httpx)
```python
import httpx

r = httpx.post("http://127.0.0.1:3112/v1/chat/completions", json={
    "model": "mistral-7b-instruct-v0.2.Q4_0.gguf",
    "messages": [{"role": "user", "content": "¬øQu√© es Python?"}],
    "max_tokens": 200
}, timeout=120.0)
print(r.json())
```

### Con OpenAI SDK
```python
from openai import OpenAI

client = OpenAI(base_url="http://127.0.0.1:3112/v1", api_key="no-key")

response = client.chat.completions.create(
    model="mistral-7b-instruct-v0.2.Q4_0.gguf",
    messages=[{"role": "user", "content": "Hola"}],
    max_tokens=100,
)
print(response.choices[0].message.content)
```

---

## ‚öôÔ∏è Referencia de variables de entorno

| Variable | Default | Descripci√≥n |
|---|---|---|
| `SERVER_HOST` | `0.0.0.0` | Host donde escucha el proxy |
| `SERVER_PORT` | `3112` | Puerto del proxy |
| `LLM_BACKEND_URL` | `http://127.0.0.1:8080` | URL donde corre llama-server |
| `LLM_MODEL_NAME` | *(vac√≠o)* | Nombre del modelo LLM por defecto |
| `LLM_START_COMMAND` | *(vac√≠o)* | Comando completo para arrancar llama-server |
| `LLM_STOP_COMMAND` | *(vac√≠o)* | Comando para detener llama-server |
| `WHISPER_BACKEND_URL` | `http://127.0.0.1:8081` | URL donde corre whisper-server |
| `WHISPER_MODEL_NAME` | *(vac√≠o)* | Nombre del modelo Whisper |
| `WHISPER_START_COMMAND` | *(vac√≠o)* | Comando completo para arrancar whisper-server |
| `WHISPER_STOP_COMMAND` | *(vac√≠o)* | Comando para detener whisper-server |
| `AUTO_SWITCH_BACKEND` | `true` | Cambiar backend autom√°ticamente seg√∫n endpoint |
| `RESPONSES_MODE` | `map` | `map` = redirige a chat/completions; `proxy` = reenv√≠a directo |
| `MODELS_DIR` | `C:\models` | Carpeta de modelos GGUF |
| `HF_TOKEN` | *(vac√≠o)* | Token de Hugging Face para repos privados |
| `MODELS_LIST_MODE` | `active` | `active` / `local` / `both` ‚Äî qu√© devuelve `/v1/models` |
| `LLM_HEALTH_PATH` | `/v1/models` | Ruta para verificar que LLM est√° listo |
| `WHISPER_HEALTH_PATH` | `/v1/models` | Ruta para verificar que Whisper est√° listo |
| `SWITCH_TIMEOUT_SECONDS` | `30` | Segundos m√°ximos esperando que un backend arranque |

---

## üõ†Ô∏è Scripts incluidos

| Script | Descripci√≥n |
|---|---|
| `scripts\start_server.bat` | Doble clic para iniciar (llama a PowerShell internamente) |
| `scripts\stop_server.bat` | Doble clic para detener todo (proxy + backends + liberar VRAM) |
| `scripts\start_server.ps1` | Script principal. Uso: `-Action start` o `-Action stop`. Mata procesos previos, busca Python, arranca llama-server si est√° configurado, e inicia uvicorn. |
| `scripts\stop_server.ps1` | Alias que llama a `start_server.ps1 -Action stop` |
| `scripts\setup_check.ps1` | Verifica que existen todos los ejecutables, modelos y carpetas del `.env` |
| `scripts\test_llm.py` | Smoke test: prueba `/status`, `/v1/chat/completions`, `/v1/completions` y `/v1/embeddings` |

---

## üêõ Soluci√≥n de problemas

| Problema | Soluci√≥n |
|---|---|
| `backend no disponible` (502) | Verificar que `LLM_START_COMMAND` / `WHISPER_START_COMMAND` apuntan al ejecutable correcto y que el modelo existe. |
| El servidor arranca pero no responde | Comprobar el puerto con `netstat -ano \| findstr :3112` y que no haya otro proceso us√°ndolo. |
| VRAM insuficiente | Usar un modelo con cuantizaci√≥n m√°s agresiva (Q4_0 < Q4_K_M < Q6_K < Q8_0). La UI muestra estimaci√≥n de VRAM. |
| `cambio autom√°tico deshabilitado` (409) | Establecer `AUTO_SWITCH_BACKEND=true` en `.env` o reiniciar manualmente el backend. |
| No detecta la GPU en la UI | Verificar que `nvidia-smi` est√° en el PATH. Instalar/actualizar drivers NVIDIA. |
| Error de timeout al cambiar backend | Aumentar `SWITCH_TIMEOUT_SECONDS` en `.env` (modelos grandes tardan m√°s en cargar). |
| Puerto ocupado al iniciar | Ejecutar `scripts\stop_server.bat` primero, o usar `scripts\start_server.ps1` que limpia autom√°ticamente. |

---

## üì¶ Crear paquete instalable y distribuir

### Paso 1: Generar el paquete

Desde la ra√≠z del proyecto, ejecuta:

```powershell
powershell -ExecutionPolicy Bypass -File scripts\build_package.ps1
```

Esto genera `dist\meigahub-server\` con todo lo necesario **excepto modelos y datos personales**:

| Incluido | No incluido (excluido) |
|---|---|
| `app\` ‚Äî c√≥digo del servidor | `.env` ‚Äî se genera al instalar con rutas del usuario |
| `scripts\` ‚Äî inicio/parada/test | `_gpu_test.txt` ‚Äî info personal de GPU |
| `apps\` ‚Äî llama.cpp + whisper.cpp | `__pycache__` ‚Äî cache compilado |
| `.env.example` ‚Äî plantilla | `models\*.gguf` ‚Äî demasiado grandes (~2-10 GB c/u) |
| `installer.ps1` ‚Äî instalador completo | |
| `INSTALAR.bat` ‚Äî doble clic para instalar | |

Opcionalmente lo comprime en `dist\meigahub-server.zip`.

### Paso 2: El usuario final instala

El usuario copia la carpeta o descomprime el ZIP y hace doble clic en **`INSTALAR.bat`**.

El instalador hace todo autom√°ticamente:

1. **Pregunta carpeta de instalaci√≥n** (default: `C:\MeigaHub`)
2. **Detecta Python 3.10+** ‚Äî si no lo encuentra, ofrece descargarlo e instalarlo autom√°ticamente
3. **Copia todos los archivos** a la carpeta elegida (server, backends, scripts)
4. **Instala dependencias Python** (`pip install -r requirements.txt`)
5. **Genera `.env` configurado** ‚Äî reemplaza `__INSTALL_DIR__` por la ruta real de instalaci√≥n
6. **Detecta GPU NVIDIA** ‚Äî si encuentra CUDA, configura llama.cpp-cuda autom√°ticamente
7. **Verifica la instalaci√≥n** ‚Äî comprueba que todo est√° en su sitio
8. **Crea acceso directo** en el Escritorio (opcional)

> **Nota:** Los modelos GGUF no se incluyen por su tama√±o. Tras instalar, el usuario los descarga desde la UI web en `/ui/models` o manualmente a la carpeta `models\`.

---

## üìÑ Licencia

MIT
